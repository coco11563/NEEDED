{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d19c96a0-25ce-4707-a300-2365e21f90b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import re\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer,BertConfig\n",
    "from pandas.core.frame import DataFrame\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import randn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300416c5-014e-4e97-9020-5c7223070b67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e361705-1fa7-4d73-b102-8e5c44a30366",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_path = 'hfl/chinese-bert-wwm-ext'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1436e1b3-9b22-49a9-a02b-f9901116b8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "PARTITION = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aaf4e061-ed01-455a-9b08-385aaed3a11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#jieba.load_userdict('./data/diag_dict.txt') \n",
    "#data_name = 'train'\n",
    "#data_name = 'dev'\n",
    "data_name = 'test'\n",
    "\n",
    "data_path = './data/hospital_{}_df_{}.pkl'.format(data_name, PARTITION)\n",
    "sample_df = pd.read_pickle(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7aaad7a3-f407-4496-9464-de093b117a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df['no_sym_describe'].fillna(\" \",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "79b3681f-6b84-4b3e-8d8b-257098f9be5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将各个特征分词，加上special token，并转为fillnat_ids\n",
    "\n",
    "sample_df['sym_input_ids'] = sample_df['sym_describe'].apply(lambda x : tokenizer.encode(x,add_special_tokens = True,max_length = MAX_LEN,\n",
    "                                                                                             truncation = True))\n",
    "sample_df['no_sym_input_ids'] = sample_df['no_sym_describe'].apply(lambda x : tokenizer.encode(x,add_special_tokens = True,max_length = MAX_LEN,\n",
    "                                                                                             truncation = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "14a06c67-4170-4e69-9762-56e6bc8a0c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得各个特征的attention_mask\n",
    "sample_df['sym_attention_mask'] = sample_df['sym_input_ids'].apply(lambda x : len(x) * [1] + (MAX_LEN - len(x)) * [0] )\n",
    "sample_df['no_sym_attention_mask'] = sample_df['no_sym_input_ids'].apply(lambda x : len(x) * [1] + (MAX_LEN - len(x)) * [0] )\n",
    "\n",
    "# 将xxx_input_ids填充\n",
    "sample_df['sym_input_ids'] = sample_df['sym_input_ids'].apply(lambda x : x + (MAX_LEN - len(x)) * [0] )\n",
    "sample_df['no_sym_input_ids'] = sample_df['no_sym_input_ids'].apply(lambda x : x + (MAX_LEN - len(x)) * [0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6aa58ceb-21ce-4134-9d80-35e074216a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定位sep的索引，从而获得token_type_ids,0留给cls\n",
    "def get_token_type_ids(sym_input_ids):\n",
    "    token_type_ids = [0]\n",
    "    loc_list = [0] + list(np.where(np.array(sym_input_ids) == 102)[0])\n",
    "    for i in range(len(loc_list) - 1):\n",
    "        token_type_ids += [i + 1] * (loc_list[i + 1] - loc_list[i])\n",
    "    \n",
    "    token_type_ids += (MAX_LEN - len(token_type_ids)) * [-1]  \n",
    "    return token_type_ids\n",
    "\n",
    "# 获得各部分token_type从而区分organ\n",
    "\n",
    "sample_df['sym_token_type_ids'] = sample_df['sym_input_ids'].apply(lambda x : get_token_type_ids(x))\n",
    "sample_df['no_sym_token_type_ids'] = sample_df['no_sym_input_ids'].apply(lambda x : get_token_type_ids(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667231b7-2942-40e3-a3fd-397902906c48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a54400bf-ba6c-48f5-988e-dd1366baf360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机打乱样本顺序\n",
    "sample_df = sample_df.sample(n = len(sample_df),random_state = 42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7b620a0f-dd4a-43f4-bd44-ba12b9a71715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_254880/2035788198.py:12: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  all_labels = torch.tensor(sample_df['label'], dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "# 构建tensordataset数据集并保存\n",
    "sym_input_ids = torch.tensor(list(sample_df['sym_input_ids']), dtype=torch.long)\n",
    "no_sym_input_ids = torch.tensor(list(sample_df['no_sym_input_ids']), dtype=torch.long)\n",
    "\n",
    "sym_attention_mask = torch.tensor(list(sample_df['sym_attention_mask']), dtype=torch.long)\n",
    "no_sym_attention_mask = torch.tensor(list(sample_df['no_sym_attention_mask']), dtype=torch.long)\n",
    "\n",
    "\n",
    "sym_token_type_ids = torch.tensor(list(sample_df['sym_token_type_ids']), dtype=torch.long)\n",
    "no_sym_token_type_ids = torch.tensor(list(sample_df['no_sym_token_type_ids']), dtype=torch.long)\n",
    "\n",
    "all_labels = torch.tensor(sample_df['label'], dtype=torch.long)\n",
    "\n",
    "all_dataset = TensorDataset(sym_input_ids, no_sym_input_ids, sym_attention_mask, \n",
    "                            no_sym_attention_mask, sym_token_type_ids, \n",
    "                            no_sym_token_type_ids, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "80a204f7-17b3-4eb7-877b-72a83c01d042",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(all_dataset,'./data2/hospital_{}_dataset_{}_{}'.format(data_name,MAX_LEN,PARTITION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202a732f-0b34-4570-9fe7-55ffdf8377ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2dbedf-cf72-4e62-9106-0c6d5cce6ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36851cce-4a7c-4a6a-b150-cc46bbb5a042",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80541cdf-4577-483d-898c-4fd27eacfbf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237603b8-7a61-43f0-b052-cdc4008f64af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
